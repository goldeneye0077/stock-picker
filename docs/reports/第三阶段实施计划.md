# 数据采集优化第三阶段实施计划

## 一、阶段概述

**实施阶段**：第三阶段 - 架构优化
**实施时间**：2025年12月10日 - 2025年12月31日（3周）
**实施目标**：建立可持续的数据采集架构，提高系统可靠性和效率
**总体评分目标**：从57.2分提升到85分+

## 二、第三阶段核心任务

### 2.1 增量更新实现 ✅ 优先级：高
**目标**：避免重复采集，提高效率，只采集新增数据

**实施步骤：**
1. **设计增量采集架构**
   - 创建`IncrementalDataCollector`类
   - 实现上次采集时间记录机制
   - 设计增量数据检测算法

2. **实现增量采集逻辑**
   ```python
   class IncrementalDataCollector:
       def __init__(self):
           self.last_collection_date = self.get_last_collection_date()

       async def collect_incremental(self):
           new_dates = self.get_new_dates_since_last()
           for date in new_dates:
               await self.collect_date_data(date)
   ```

3. **集成到现有系统**
   - 修改前端设置页面，增加"增量更新"选项
   - 更新后端API，支持增量采集模式
   - 测试增量采集功能

**预期效果：**
- API调用次数减少50%+
- 采集时间缩短40%+
- 避免数据重复，提高数据质量

### 2.2 数据质量监控体系 ✅ 优先级：高
**目标**：建立持续的数据质量保障机制

**实施步骤：**
1. **设计监控指标体系**
   - 覆盖率指标：股票覆盖率、时间覆盖率、数据字段覆盖率
   - 完整性指标：数据缺失率、数据错误率、数据一致性
   - 时效性指标：采集延迟、更新频率、数据新鲜度

2. **实现监控工具**
   - 创建`DataQualityMonitor`类
   - 实现定时检查任务
   - 设计实时报警机制
   - 实现质量报告生成

3. **建立问题自动修复机制**
   - 数据缺失自动补全
   - 数据错误自动修正
   - 数据不一致自动同步

**预期效果：**
- 数据质量评分提升到85分+
- 问题发现时间缩短到1小时内
- 自动修复率达到80%+

### 2.3 多数据源备份 ✅ 优先级：中
**目标**：提高数据可靠性和可用性

**实施步骤：**
1. **设计多数据源架构**
   - 主数据源：Tushare Pro（当前使用）
   - 备用数据源1：AKShare
   - 备用数据源2：Baostock
   - 备用数据源3：其他免费数据源

2. **实现数据源切换策略**
   ```python
   class MultiSourceDataCollector:
       def __init__(self):
           self.primary_source = TushareClient()
           self.backup_sources = [AKShareClient(), BaostockClient()]

       async def collect_with_fallback(self, func_name, *args):
           try:
               return await getattr(self.primary_source, func_name)(*args)
           except Exception as e:
               for backup in self.backup_sources:
                   try:
                       return await getattr(backup, func_name)(*args)
                   except:
                       continue
               raise
   ```

3. **实现数据一致性验证**
   - 多源数据对比验证
   - 数据源健康检查
   - 自动切换和恢复

**预期效果：**
- 系统可用性达到99.9%+
- 数据源故障自动切换
- 数据一致性验证覆盖率100%

## 三、技术实现细节

### 3.1 增量更新实现方案

#### 3.1.1 数据库设计
```sql
-- 新增采集历史表
CREATE TABLE IF NOT EXISTS collection_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    collection_type TEXT NOT NULL,  -- 'full' 或 'incremental'
    start_date TEXT NOT NULL,
    end_date TEXT NOT NULL,
    stock_count INTEGER DEFAULT 0,
    kline_count INTEGER DEFAULT 0,
    flow_count INTEGER DEFAULT 0,
    status TEXT DEFAULT 'completed',  -- 'pending', 'running', 'completed', 'failed'
    error_message TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### 3.1.2 增量检测算法
```python
async def get_new_dates_since_last(self) -> List[str]:
    """获取上次采集后的新交易日"""
    last_date = self.get_last_collection_date()
    if not last_date:
        return await self.get_trading_days(days=7)  # 首次采集

    all_dates = await self.get_trading_days(days=30)  # 获取最近30天
    new_dates = [date for date in all_dates if date > last_date]
    return new_dates[:7]  # 最多采集7天
```

### 3.2 质量监控体系实现

#### 3.2.1 监控指标计算
```python
class DataQualityMetrics:
    async def calculate_coverage(self) -> Dict[str, float]:
        """计算数据覆盖率"""
        metrics = {
            'stock_coverage': self._calc_stock_coverage(),
            'time_coverage': self._calc_time_coverage(),
            'field_coverage': self._calc_field_coverage(),
            'hot_stock_coverage': self._calc_hot_stock_coverage()
        }
        return metrics

    async def calculate_completeness(self) -> Dict[str, float]:
        """计算数据完整性"""
        metrics = {
            'missing_rate': self._calc_missing_rate(),
            'error_rate': self._calc_error_rate(),
            'consistency_score': self._calc_consistency_score()
        }
        return metrics
```

#### 3.2.2 报警机制
```python
class DataQualityAlert:
    def __init__(self):
        self.thresholds = {
            'stock_coverage': 0.95,  # 低于95%报警
            'time_coverage': 0.90,   # 低于90%报警
            'hot_stock_coverage': 1.0,  # 必须100%
            'missing_rate': 0.05,    # 高于5%报警
        }

    async def check_and_alert(self, metrics: Dict[str, float]):
        """检查指标并触发报警"""
        alerts = []
        for metric_name, threshold in self.thresholds.items():
            value = metrics.get(metric_name, 0)
            if metric_name.endswith('_rate'):
                if value > threshold:
                    alerts.append(f"{metric_name}: {value:.1%} > {threshold:.1%}")
            else:
                if value < threshold:
                    alerts.append(f"{metric_name}: {value:.1%} < {threshold:.1%}")

        if alerts:
            await self.send_alert(alerts)
```

### 3.3 多数据源实现

#### 3.3.1 数据源抽象接口
```python
from abc import ABC, abstractmethod

class DataSource(ABC):
    """数据源抽象基类"""

    @abstractmethod
    async def get_stock_basic(self) -> Optional[pd.DataFrame]:
        pass

    @abstractmethod
    async def get_daily_data_by_date(self, trade_date: str) -> Optional[pd.DataFrame]:
        pass

    @abstractmethod
    async def get_moneyflow_by_date(self, trade_date: str) -> Optional[pd.DataFrame]:
        pass

    @abstractmethod
    def is_available(self) -> bool:
        pass
```

#### 3.3.2 数据源健康检查
```python
class DataSourceHealthChecker:
    async def check_health(self, source: DataSource) -> Dict[str, any]:
        """检查数据源健康状态"""
        health = {
            'available': source.is_available(),
            'latency': await self._measure_latency(source),
            'success_rate': await self._calc_success_rate(source),
            'data_freshness': await self._check_data_freshness(source)
        }
        return health
```

## 四、实施时间表

### 4.1 第1周（2025-12-10 至 2025-12-17）
| 任务 | 负责人 | 状态 | 备注 |
|------|--------|------|------|
| 增量更新架构设计 | 开发团队 | 📅 待开始 | 设计数据库表和算法 |
| 增量采集核心实现 | 开发团队 | 📅 待开始 | 实现增量检测和采集 |
| 前端增量更新界面 | 前端团队 | 📅 待开始 | 增加增量更新选项 |

### 4.2 第2周（2025-12-18 至 2025-12-24）
| 任务 | 负责人 | 状态 | 备注 |
|------|--------|------|------|
| 质量监控体系设计 | 开发团队 | 📅 待开始 | 设计监控指标和报警规则 |
| 监控工具实现 | 开发团队 | 📅 待开始 | 实现定时检查和报告生成 |
| 多数据源架构设计 | 开发团队 | 📅 待开始 | 设计数据源抽象接口 |

### 4.3 第3周（2025-12-25 至 2025-12-31）
| 任务 | 负责人 | 状态 | 备注 |
|------|--------|------|------|
| 多数据源实现 | 开发团队 | 📅 待开始 | 实现AKShare和Baostock客户端 |
| 数据源切换策略 | 开发团队 | 📅 待开始 | 实现自动切换和健康检查 |
| 系统集成测试 | QA团队 | 📅 待开始 | 全流程测试和性能测试 |

## 五、预期效果和验收标准

### 5.1 性能指标提升
| 指标 | 当前值 | 目标值 | 提升幅度 |
|------|--------|--------|----------|
| 数据质量评分 | 57.2分 | 85分+ | +27.8分 |
| 采集时间 | 58.6秒 | <30秒 | -48.8% |
| API调用次数 | 15次/7天 | 8次/7天 | -46.7% |
| 系统可用性 | 未监控 | 99.9%+ | 新增监控 |

### 5.2 功能验收标准
1. **增量更新功能** ✅
   - 支持增量采集模式
   - 避免数据重复采集
   - 增量检测准确率>99%

2. **质量监控体系** ✅
   - 定时检查任务正常运行
   - 实时报警机制有效
   - 质量报告自动生成

3. **多数据源备份** ✅
   - 主数据源故障时自动切换
   - 数据一致性验证通过
   - 备用数据源可用性>95%

### 5.3 业务价值
1. **提高运维效率**：自动化监控减少人工检查
2. **降低运营成本**：减少API调用次数和数据存储
3. **增强系统可靠性**：多数据源保障数据可用性
4. **支持业务扩展**：为实时数据更新奠定基础

## 六、风险控制和应对措施

### 6.1 技术风险
| 风险类型 | 可能影响 | 应对措施 | 优先级 |
|----------|----------|----------|--------|
| 增量检测算法错误 | 数据遗漏或重复 | 1. 算法测试覆盖<br>2. 人工验证机制<br>3. 回滚方案 | 高 |
| 监控系统性能问题 | 系统负载增加 | 1. 异步处理<br>2. 分批检查<br>3. 性能优化 | 中 |
| 多数据源接口变更 | 数据采集失败 | 1. 接口抽象层<br>2. 版本兼容<br>3. 快速适配 | 高 |

### 6.2 业务风险
| 风险类型 | 可能影响 | 应对措施 | 优先级 |
|----------|----------|----------|--------|
| 数据延迟增加 | 分析决策滞后 | 1. 优先级调度<br>2. 并行处理<br>3. 延迟报警 | 中 |
| 系统复杂度增加 | 维护难度提高 | 1. 文档完善<br>2. 模块化设计<br>3. 自动化测试 | 中 |

## 七、资源需求

### 7.1 人力资源
| 角色 | 数量 | 主要职责 | 参与时间 |
|------|------|----------|----------|
| 后端开发工程师 | 2人 | 架构设计和核心实现 | 全阶段 |
| 前端开发工程师 | 1人 | 界面更新和用户体验 | 第1周 |
| 测试工程师 | 1人 | 功能测试和性能测试 | 第3周 |
| 运维工程师 | 1人 | 部署监控和故障处理 | 第2-3周 |

### 7.2 技术资源
| 资源类型 | 规格要求 | 用途 |
|----------|----------|------|
| 开发服务器 | 4核8G | 开发测试环境 |
| 测试数据库 | SSD存储 | 性能测试数据 |
| 监控工具 | 开源方案 | 系统监控和报警 |

## 八、下一步行动

### 8.1 立即行动（第1周）
1. **启动增量更新实现**
   - 创建`IncrementalDataCollector`类框架
   - 设计数据库表结构
   - 实现增量检测算法原型

2. **更新项目计划**
   - 将第三阶段任务加入项目管理工具
   - 分配具体负责人和截止日期
   - 建立周度评审机制

### 8.2 中期准备（第2周）
1. **质量监控体系设计评审**
2. **多数据源技术选型确认**
3. **测试用例设计完成**

### 8.3 后期验收（第3周）
1. **系统集成测试**
2. **性能基准测试**
3. **用户验收测试**

---

**计划创建时间**：2025-12-10
**计划版本**：v1.0
**负责人**：开发团队

**备注**：本计划基于第二阶段实施成果，重点解决数据采集的效率、质量和可靠性问题，为第四阶段的高级功能（实时数据更新、自动化运维、数据版本管理）奠定基础。